{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "880c0293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "\n",
    "class TrackEmbeddingUpdater(nn.Module):\n",
    "    def __init__(self, num_cameras, embed_dim=256, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_cameras = num_cameras\n",
    "        self.cross_attn = nn.ModuleList([\n",
    "            nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "            for _ in range(num_cameras)\n",
    "        ])\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, track_embeds, detection_embeds):\n",
    "        attn_outputs = []\n",
    "        for cam_idx in range(self.num_cameras):\n",
    "            attn_out, _ = self.cross_attn[cam_idx](\n",
    "                query=track_embeds,\n",
    "                key=detection_embeds[cam_idx],\n",
    "                value=detection_embeds[cam_idx]\n",
    "            )\n",
    "            attn_outputs.append(attn_out)\n",
    "        aggregated = torch.mean(torch.stack(attn_outputs), dim=0)\n",
    "        self_attn_out, _ = self.self_attn(aggregated, aggregated, aggregated)\n",
    "        return self.ffn(self_attn_out)\n",
    "\n",
    "class AssociationModule(nn.Module):\n",
    "    def __init__(self, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.detection_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.track_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.embed_dim = embed_dim\n",
    "    def forward(self, detection_embeds, track_embeds):\n",
    "        Q = self.detection_proj(detection_embeds)\n",
    "        K = self.track_proj(track_embeds)\n",
    "        return torch.softmax(torch.matmul(Q, K.transpose(1,2)) / (self.embed_dim**0.5), dim=-1)\n",
    "\n",
    "class MCTR(nn.Module):\n",
    "    def __init__(self, num_cameras=4, num_queries=100, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.detr = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        self.processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        \n",
    "        self.track_queries = nn.Embedding(num_queries, embed_dim)\n",
    "        self.track_updater = TrackEmbeddingUpdater(num_cameras, embed_dim)\n",
    "        self.associator = AssociationModule(embed_dim)\n",
    "        \n",
    "        self.detection_proj = nn.Linear(self.detr.config.d_model, embed_dim)\n",
    "        self.register_buffer('active_tracks', None)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.track_queries.weight)\n",
    "\n",
    "    def forward(self, multi_camera_views):\n",
    "        print(len(multi_camera_views), \"Number of camera views:\", len(multi_camera_views))\n",
    "        print(\"Shape of first camera view:\", multi_camera_views[0])\n",
    "        batch_size = len(multi_camera_views)\n",
    "        \n",
    "        # Inicjalizacja śladów\n",
    "        if self.active_tracks is None:\n",
    "            self.active_tracks = self.track_queries.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "        # Przetwarzanie DETR dla każdej kamery\n",
    "        detection_embeds = []\n",
    "        for view in multi_camera_views:\n",
    "            inputs = self.processor(images=view, return_tensors=\"pt\").to(view.device)\n",
    "            outputs = self.detr(**inputs)\n",
    "            detection_embeds.append(self.detection_proj(outputs.last_hidden_state))\n",
    "\n",
    "        # Aktualizacja track embeddings\n",
    "        updated_tracks = self.track_updater(self.active_tracks, detection_embeds)\n",
    "        \n",
    "        # Asocjacja detekcji\n",
    "        assignments = [self.associator(d, updated_tracks) for d in detection_embeds]\n",
    "\n",
    "        self.active_tracks = updated_tracks.detach()\n",
    "        return {\n",
    "            'tracks': updated_tracks,\n",
    "            'assignments': assignments,\n",
    "            'detection_embeds': detection_embeds\n",
    "        }\n",
    "\n",
    "class MCTRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.detr_loss = nn.CrossEntropyLoss()\n",
    "        self.track_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "    def forward(self, outputs, targets):\n",
    "        # Strata detekcji\n",
    "        detr_loss = self.detr_loss(\n",
    "            outputs['detection_logits'], \n",
    "            targets['detection_labels']\n",
    "        )\n",
    "\n",
    "        # Strata asocjacji\n",
    "        track_loss = 0\n",
    "        for cam_assignment, gt_assignment in zip(outputs['assignments'], targets['assignments']):\n",
    "            track_loss += self.track_loss(\n",
    "                torch.log(cam_assignment),\n",
    "                gt_assignment\n",
    "            )\n",
    "\n",
    "        return detr_loss + track_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1488ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DetrImageProcessor\n",
    "\n",
    "class WildtrackDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 annotations_dir: str,\n",
    "                 images_base_dir: str,\n",
    "                 cameras: dict = {0: 'C1', 5: 'C6'},\n",
    "                 frame_step: int = 5,\n",
    "                 max_detections: int = 50):\n",
    "        \n",
    "        self.cameras = cameras\n",
    "        self.frame_step = frame_step\n",
    "        self.max_detections = max_detections\n",
    "\n",
    "        # Parsowanie nazw plików JSON: np. 00000000.json, 00000005.json, ...\n",
    "        self.frame_files = []\n",
    "        for f in os.listdir(annotations_dir):\n",
    "            if f.endswith('.json'):\n",
    "                try:\n",
    "                    frame_number = int(os.path.splitext(f)[0])\n",
    "                    self.frame_files.append((f, frame_number))\n",
    "                except Exception as e:\n",
    "                    print(f\"Ostrzeżenie: Pominięto plik {f} - nieprawidłowy format nazwy\")\n",
    "        \n",
    "        # Sortuj po numerze klatki\n",
    "        self.frame_files.sort(key=lambda x: x[1])\n",
    "        self.frame_files = [f[0] for f in self.frame_files]\n",
    "\n",
    "        # Wczytaj adnotacje\n",
    "        self.annotations = []\n",
    "        for frame_file in self.frame_files:\n",
    "            with open(os.path.join(annotations_dir, frame_file)) as f:\n",
    "                frame_data = json.load(f)\n",
    "                self.annotations.append(self._process_frame(frame_data))\n",
    "        \n",
    "        # Przygotuj mapowanie ścieżek do obrazów\n",
    "        self.image_paths = {}\n",
    "        for view_num, cam_id in self.cameras.items():\n",
    "            cam_dir = os.path.join(images_base_dir, cam_id)\n",
    "            self.image_paths[view_num] = {\n",
    "                int(os.path.splitext(f)[0]): os.path.join(cam_dir, f)\n",
    "                for f in os.listdir(cam_dir) \n",
    "                if f.endswith('.png')\n",
    "            }\n",
    "\n",
    "        self.processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "    def _process_frame(self, frame_data):\n",
    "        \"\"\"Przetwarza surowe dane z pliku JSON na format per kamera\"\"\"\n",
    "        frame_anns = {view_num: [] for view_num in self.cameras.keys()}\n",
    "        \n",
    "        for person in frame_data:\n",
    "            for view in person['views']:\n",
    "                view_num = view['viewNum']\n",
    "                if view_num in self.cameras and view['xmin'] != -1:\n",
    "                    bbox = [\n",
    "                        view['xmin'], \n",
    "                        view['ymin'], \n",
    "                        view['xmax'] - view['xmin'], \n",
    "                        view['ymax'] - view['ymin']\n",
    "                    ]\n",
    "                    frame_anns[view_num].append({\n",
    "                        'bbox': bbox,\n",
    "                        'track_id': person['personID']\n",
    "                    })\n",
    "        \n",
    "        return frame_anns\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Numer klatki wg nazwy pliku (np. 00000000.json → 0, 00000005.json → 5 itd.)\n",
    "        frame_number = int(os.path.splitext(self.frame_files[idx])[0])\n",
    "        \n",
    "        # Wczytaj obrazy dla wszystkich kamer\n",
    "        images = {}\n",
    "        for view_num, cam_id in self.cameras.items():\n",
    "            img_path = self.image_paths[view_num].get(frame_number)\n",
    "            if not img_path:\n",
    "                raise FileNotFoundError(f\"Brak obrazu {cam_id} dla klatki {frame_number}\")\n",
    "            \n",
    "            images[view_num] = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Pobierz adnotacje dla klatki\n",
    "        anns = self.annotations[idx]\n",
    "        \n",
    "        # Przygotuj dane wyjściowe\n",
    "        formatted_anns = {}\n",
    "        for view_num in self.cameras.keys():\n",
    "            cam_anns = anns.get(view_num, [])\n",
    "            \n",
    "            boxes = torch.zeros((self.max_detections, 4))\n",
    "            track_ids = torch.zeros(self.max_detections, dtype=torch.long)\n",
    "            \n",
    "            for i, ann in enumerate(cam_anns[:self.max_detections]):\n",
    "                boxes[i] = torch.tensor(ann['bbox'])\n",
    "                track_ids[i] = ann['track_id']\n",
    "            \n",
    "            formatted_anns[view_num] = {\n",
    "                'boxes': boxes,\n",
    "                'track_ids': track_ids\n",
    "            }\n",
    "\n",
    "        return images, formatted_anns\n",
    "\n",
    "\n",
    "def create_collate_fn(processor):\n",
    "\n",
    "    def custom_collate_fn(batch):\n",
    "        images_batch, anns_batch = zip(*batch)\n",
    "        \n",
    "        # Przetwórz obrazy przez procesor DETR\n",
    "        processed_images = {}\n",
    "        for view_num in batch[0][0].keys():\n",
    "            view_images = [img[view_num] for img in images_batch]\n",
    "            processed = processor(images=view_images, return_tensors=\"pt\")\n",
    "            processed_images[view_num] = {\n",
    "                'pixel_values': processed['pixel_values'],\n",
    "                'pixel_mask': processed['pixel_mask']\n",
    "            }\n",
    "        \n",
    "        # Przetwórz adnotacje\n",
    "        formatted_anns = {\n",
    "            'detection_labels': {},\n",
    "            'assignments': {}\n",
    "        }\n",
    "        \n",
    "        for view_num in anns_batch[0].keys():\n",
    "            all_boxes = torch.stack([anns[view_num]['boxes'] for anns in anns_batch])\n",
    "            all_track_ids = torch.stack([anns[view_num]['track_ids'] for anns in anns_batch])\n",
    "            \n",
    "            formatted_anns['detection_labels'][view_num] = all_boxes\n",
    "            formatted_anns['assignments'][view_num] = all_track_ids\n",
    "        \n",
    "        return processed_images, formatted_anns\n",
    "    return custom_collate_fn\n",
    "# Przykład użycia\n",
    "dataset = WildtrackDataset(\n",
    "    annotations_dir='Dataset/annotations_positions',\n",
    "    images_base_dir='Dataset/image_subsets',\n",
    "    cameras={0: 'C1', 5: 'C6'},  # viewNum 0 → C1, viewNum 5 → C6\n",
    "    frame_step=5\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=create_collate_fn(dataset.processor),\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99138e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/1\n",
      "tutaj1\n",
      "tutaj2\n",
      "tutaj3\n",
      "tutaj3\n",
      "tutaj4\n",
      "2 Number of camera views: 2\n",
      "Shape of first camera view: {'pixel_values': tensor([[[[-1.2959, -1.1075, -1.1247,  ...,  0.7419,  0.9132,  1.3070],\n",
      "          [-1.2617, -1.1932, -1.2274,  ...,  1.1015,  1.2728,  1.6153],\n",
      "          [-1.2445, -1.2445, -1.2617,  ...,  1.6667,  1.7523,  1.8379],\n",
      "          ...,\n",
      "          [ 0.2111,  0.1426,  0.1083,  ..., -0.6623, -0.6452, -0.6109],\n",
      "          [ 0.1597,  0.1254,  0.0912,  ..., -0.6623, -0.6452, -0.6109],\n",
      "          [ 0.0398,  0.0741,  0.0569,  ..., -0.6452, -0.6281, -0.6281]],\n",
      "\n",
      "         [[-1.1954, -1.0028, -1.0203,  ...,  0.8179,  0.9755,  1.3957],\n",
      "          [-1.1604, -1.0903, -1.1253,  ...,  1.1856,  1.3606,  1.7108],\n",
      "          [-1.1779, -1.1604, -1.1779,  ...,  1.7633,  1.8508,  1.9384],\n",
      "          ...,\n",
      "          [ 0.3627,  0.2927,  0.2577,  ..., -0.5126, -0.4951, -0.4601],\n",
      "          [ 0.3102,  0.2752,  0.2577,  ..., -0.5126, -0.4951, -0.4601],\n",
      "          [ 0.2052,  0.2402,  0.2402,  ..., -0.4951, -0.4776, -0.4776]],\n",
      "\n",
      "         [[-1.0550, -0.8633, -0.8807,  ...,  0.3219,  0.4788,  0.8797],\n",
      "          [-1.0376, -0.9678, -1.0027,  ...,  0.6531,  0.8274,  1.1759],\n",
      "          [-1.0724, -1.0550, -1.0724,  ...,  1.2282,  1.3154,  1.4025],\n",
      "          ...,\n",
      "          [ 0.3393,  0.2696,  0.2348,  ..., -0.2881, -0.2707, -0.2358],\n",
      "          [ 0.2871,  0.2522,  0.2348,  ..., -0.2881, -0.2707, -0.2358],\n",
      "          [ 0.1825,  0.2173,  0.2173,  ..., -0.2707, -0.2532, -0.2532]]]]), 'pixel_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]]])}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'int'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Avg Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     54\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmctr_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[37], line 32\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtutaj4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 32\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtutaj5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, target_tensors)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[35], line 71\u001b[0m, in \u001b[0;36mMCTR.forward\u001b[1;34m(self, multi_camera_views)\u001b[0m\n\u001b[0;32m     69\u001b[0m detection_embeds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m view \u001b[38;5;129;01min\u001b[39;00m multi_camera_views:\n\u001b[1;32m---> 71\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(view\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     72\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetr(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     73\u001b[0m     detection_embeds\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetection_proj(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[1;34m(self, images, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\detr\\image_processing_detr.py:1364\u001b[0m, in \u001b[0;36mDetrImageProcessor.preprocess\u001b[1;34m(self, images, annotations, return_segmentation_masks, masks_path, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, do_convert_annotations, image_mean, image_std, do_pad, format, return_tensors, data_format, input_data_format, pad_size, **kwargs)\u001b[0m\n\u001b[0;32m   1361\u001b[0m pad_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_size \u001b[38;5;28;01mif\u001b[39;00m pad_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pad_size\n\u001b[0;32m   1362\u001b[0m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m-> 1364\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mmake_list_of_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid_images(images):\n\u001b[0;32m   1367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\image_utils.py:205\u001b[0m, in \u001b[0;36mmake_list_of_images\u001b[1;34m(images, expected_ndims)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image shape. Expected either \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_ndims\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_ndims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, but got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         )\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m images\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax.ndarray, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid image type. Expected either PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray, but got <class 'int'>."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MCTR(num_cameras=len(dataset.cameras)).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = MCTRLoss()\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    print('tutaj1')\n",
    "    for batch_idx, (processed_images, targets) in enumerate(dataloader):\n",
    "        # Przygotuj dane wejściowe\n",
    "        inputs = {}\n",
    "        print('tutaj2')\n",
    "        for view_num in processed_images.keys():\n",
    "            inputs[view_num] = {\n",
    "                'pixel_values': processed_images[view_num]['pixel_values'].to(device),\n",
    "                'pixel_mask': processed_images[view_num]['pixel_mask'].to(device)\n",
    "            }\n",
    "            print('tutaj3')\n",
    "        # Przygotuj cele\n",
    "        target_tensors = {\n",
    "            'detection_labels': {\n",
    "                view_num: t.to(device) for view_num, t in targets['detection_labels'].items()\n",
    "            },\n",
    "            'assignments': {\n",
    "                view_num: t.to(device) for view_num, t in targets['assignments'].items()\n",
    "            }\n",
    "        }\n",
    "        print('tutaj4')\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        print('tutaj5')\n",
    "        loss = criterion(outputs, target_tensors)\n",
    "        print('tutaj6')\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        print('tutaj7')\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Batch {batch_idx}/{len(dataloader)} - Loss: {loss.item():.4f}')\n",
    "        print('tutaj8')\n",
    "    print('tutaj9')\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs}')\n",
    "    avg_loss = train_epoch(model, dataloader, optimizer, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Avg Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    torch.save(model.state_dict(), f'mctr_epoch_{epoch+1}.pth')\n",
    "\n",
    "print('Trening zakończony!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca042e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MCTR(num_cameras=len(dataset.cameras)).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = MCTRLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68956862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ad3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klatka 0:\n",
      "Kamera 0:\n",
      "  Obraz: (1920, 1080)\n",
      "  Bboxy: torch.Size([50, 4])\n",
      "Kamera 5:\n",
      "  Obraz: (1920, 1080)\n",
      "  Bboxy: torch.Size([50, 4])\n"
     ]
    }
   ],
   "source": [
    "# Krok 1: Sprawdź czy dataset zwraca poprawne dane\n",
    "sample_idx = 0\n",
    "images, anns = dataset[sample_idx]\n",
    "print(f\"Klatka {sample_idx}:\")\n",
    "for view_num in dataset.cameras:\n",
    "    print(f\"Kamera {view_num}:\")\n",
    "    print(f\"  Obraz: {images[view_num].size}\")\n",
    "    print(f\"  Bboxy: {anns[view_num]['boxes'].shape}\")\n",
    "\n",
    "# Krok 2: Sprawdź czy DataLoader zwraca batch\n",
    "try:\n",
    "    test_batch = next(iter(dataloader))\n",
    "    print(\"\\nBatch testowy:\")\n",
    "    print(f\"Liczba kamer: {len(test_batch[0])}\")\n",
    "    print(f\"Rozmiar batcha: {test_batch[0][0]['pixel_values'].shape[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Błąd w DataLoader: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "def visualize_sample(dataset, num_samples=3):\n",
    "    fig, axs = plt.subplots(num_samples, len(dataset.cameras), figsize=(15, num_samples*3))\n",
    "    if num_samples == 1:\n",
    "        axs = axs.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        idx = np.random.randint(len(dataset))\n",
    "        images, anns = dataset[idx]\n",
    "        \n",
    "        for cam_idx, (view_num, cam_id) in enumerate(dataset.cameras.items()):\n",
    "            ax = axs[i, cam_idx] if num_samples > 1 else axs[cam_idx]\n",
    "            img = np.array(images[view_num])\n",
    "            \n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f'Klatka {idx} - {cam_id}')\n",
    "            \n",
    "            for box, track_id in zip(anns[view_num]['boxes'], anns[view_num]['track_ids']):\n",
    "                if track_id == 0 or torch.all(box == 0):\n",
    "                    continue\n",
    "                x, y, w, h = box.numpy()\n",
    "                rect = patches.Rectangle(\n",
    "                    (x, y), w, h,\n",
    "                    linewidth=1,\n",
    "                    edgecolor='r',\n",
    "                    facecolor='none'\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x, y, f'ID: {track_id}', color='white', backgroundcolor='red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_sample(dataset, num_samples=3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925eb904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "dictt = {\n",
    "    'a': 1,\n",
    "    'b': 2,\n",
    "    'c': 3\n",
    "}\n",
    "print(len(dictt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
